{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d583af2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "df = pd.read_csv('Final_semtiment_document(2).csv')\n",
    "nltk.download([\"names\",\"stopwords\",\"twitter_samples\", \"averaged_perceptron_tagger\",\"vader_lexicon\",\"punkt\"])\n",
    "\n",
    "\n",
    " \n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "Clean_tweet=[]\n",
    "def clean(df):\n",
    "    punc = '''!(-[{;:'\"\\/|<>,.?@#$%^&*_~`}])'''\n",
    "    i =0\n",
    "    for row in tqdm(df['content'], desc=\"cleaning\", total=len(df)):\n",
    "        row = re.sub(r'@[A-Za-z0-9]+', '', row) \n",
    "        row = re.sub(r'#', '', row) \n",
    "        row = re.sub(r'\\n', '',row) \n",
    "        row = re.sub(r'https?:\\/\\/\\S+', '', row)\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        row= emoji_pattern.sub(r'', row) \n",
    "\n",
    "        for word in row:\n",
    "                for char in word:\n",
    "                    if char in punc:\n",
    "                        row = row.replace(char, \" \")\n",
    "            \n",
    "        text_tokens = word_tokenize(row.lower())\n",
    "        tokens_without_sw = [word for word in text_tokens if word not in stopwords]\n",
    "        row = (\" \").join(tokens_without_sw)\n",
    "        \n",
    "\n",
    "          \n",
    "        Clean_tweet.append(row)\n",
    "        i+= 1\n",
    "    Clean_tweet_df =pd.DataFrame(Clean_tweet, columns= [\"Clean_tweet\"])\n",
    "    df=pd.concat([df.reset_index(drop=True),Clean_tweet_df],axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4700f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleaning: 100%|████████████████████████████████████████████████████████████████| 38537/38537 [00:12<00:00, 3126.20it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_without_stopwords = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957d634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     code generating ai introduce security vulnerab...\n",
       "1     advanced prompt engineering also needs kind se...\n",
       "2     chatgpt big deal ’ why… digital security ai ai...\n",
       "3     chatgpt big deal ’ why… digital security ai ai...\n",
       "4     ’ chat gpt rabbit hole ’ fucking scared ’ maki...\n",
       "5     security attack chatgpt artificialintelligence...\n",
       "6     code generating ai introduce security vulnerab...\n",
       "7     published github action poc openai checks modi...\n",
       "8     used chatgpt generate first drafts compliance ...\n",
       "9     ’ played around chatgpt chatgpt3 truly impress...\n",
       "10    rising inception dockers containers kubernetes...\n",
       "11    financial literacykey wealth securityinvest kn...\n",
       "12    starting get worried job security rest assured...\n",
       "13    many talks security cons 2023 include least on...\n",
       "14    new post connor mccrory amazon security lake w...\n",
       "15    digital agents digital visions britaingetsingi...\n",
       "16    bitcoin features bitcoin cryptocurrencies incl...\n",
       "17    chatgptlearnwhatearns able minimize security v...\n",
       "18              chatgpt threat tool ai chatgpt security\n",
       "19    use chat gpt make money online 2023 bugbounty ...\n",
       "Name: Clean_tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_stopwords['Clean_tweet'][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a768ae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'Text_new', 'Datecreated', 'Month', 'VADER_Scores',\n",
       "       'VADER_Polarity', 'VADER_Sentiment', 'Topic_Name', 'Sentiment Scores',\n",
       "       'Sentiment Labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461d41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!(-[{;:'\"\\/|<>,.?@#$%^&*_~`}])'''\n",
    "\n",
    "df_without_stopwords['Clean_tweet'] = df_without_stopwords['Clean_tweet'].apply(lambda x: re.sub(punc, '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700ee5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "df_without_stopwords['Tweet_tokenized'] = df_without_stopwords['Clean_tweet'].apply(lambda x: re.split('\\W+', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52be7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [code, generating, ai, introduce, security, vu...\n",
       "1        [advanced, prompt, engineering, also, needs, k...\n",
       "2        [chatgpt, big, deal, why, digital, security, a...\n",
       "3        [chatgpt, big, deal, why, digital, security, a...\n",
       "4        [, chat, gpt, rabbit, hole, fucking, scared, m...\n",
       "                               ...                        \n",
       "77069    [creating, solidity, rust, clarity, contract, ...\n",
       "77070    [saw, someone, get, rubberduckyscript, chatgpt...\n",
       "77071    [chatgpt, fall, political, compass, compass, b...\n",
       "77072    [github, trending, archive, 28, dec, 2022, go,...\n",
       "77073    [new, top, story, hacker, news, chatgpt, polit...\n",
       "Name: Tweet_tokenized, Length: 77074, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_stopwords['Tweet_tokenized'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c367b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Menaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86abe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "word_net_lem = WordNetLemmatizer()\n",
    "def get_wordnet(pos):\n",
    "    if pos.startswith('N'): \n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN     \n",
    "\n",
    "    \n",
    "\n",
    "def lemmatizer(text):\n",
    "    pos_tags = pos_tag(text)\n",
    "    text = [word_net_lem.lemmatize(word, pos = get_wordnet(pos)) for word, pos in pos_tags]\n",
    "    return text\n",
    "\n",
    "\n",
    "df_without_stopwords['Tweet_lemmatized'] = df_without_stopwords['Tweet_tokenized'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4588a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [code, generate, ai, introduce, security, vuln...\n",
       "1    [advanced, prompt, engineering, also, need, ki...\n",
       "2    [chatgpt, big, deal, why, digital, security, a...\n",
       "3    [chatgpt, big, deal, why, digital, security, a...\n",
       "4    [, chat, gpt, rabbit, hole, fucking, scar, mak...\n",
       "5    [security, attack, chatgpt, artificialintellig...\n",
       "6    [code, generate, ai, introduce, security, vuln...\n",
       "7    [publish, github, action, poc, openai, check, ...\n",
       "8    [use, chatgpt, generate, first, draft, complia...\n",
       "9    [, play, around, chatgpt, chatgpt3, truly, imp...\n",
       "Name: Tweet_lemmatized, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_stopwords['Tweet_lemmatized'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cf7611b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'Text_new', 'Datecreated', 'Month', 'VADER_Scores',\n",
       "       'VADER_Polarity', 'VADER_Sentiment', 'Topic_Name', 'Sentiment Scores',\n",
       "       'Sentiment Labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66328f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'many talk security con 2023 include least one slide chatgpt output care guess cybersecurity openai'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_stopwords['Tokens'] = df_without_stopwords['Tweet_lemmatized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df_without_stopwords['Tokens'][13]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
